---
title: "Recipe-4-Text-Mining"
author: "aaron mamula"
date: "6/15/2020"
output: html_document
---

# {.tabset .tabset-fade .tabset-pills}

## Intro {.tabset}

### Executive Summary

This module is split into two main parts which are functionally independent from one another:

1. Tweet harvesting
2. A Sentiment Analysis example

The first part that I call "tweet harvesting" just gives a very bare bones example of pulling some tweets using the ```rtweet``` package which works through Twitter's public API. 

The second part, that I call "sentiment analysis example" shows a very simple example of using the Naive Bayes classification algorithm to predict whether a tweet is "positive", "negative", or "neutral". This portion uses a dataset from Kaggle on tweets made during the 2016 GOP debate in Ohio. Here, I also use the text mining package ```tm``` to organize tweets into a dictionary of words/terms. That dictionary of terms is then split into a training sets and testing sets and the training set is used to estimate a Naive Bayes classification model.  

### Packages 

Load a bunch of libraries for the remainder of the module. 

```{r, warning=F}
library(rtweet) # for harvesting tweets
library(tm) # for text mining
library(dplyr) 
library(tidyr)
library(data.table)
library(ggplot2)
library(ggthemes)
library(e1071) # has the naiveBayes algorithm
library(caret) # good ML package, I like the confusionMatrix() function
```

## Some Twitter Data Collection

There is an R Package called [rtweet](https://rtweet.info/) that interfaces with Twitter's API and allows users to harvest Twitter Data in R. I've used it a few times and found it pretty easy to set-up and work with. I lost interest in it mainly because Twitter's API limits data collection to the most recent 10 days or so. I'm sure there is a way to access archived data but I have not yet found a research question requiring tweets that really speaks to me.

I'm providing a pretty bare bones example here because I think some of you might be very interested in Science Communication or Public Relations type things that might be effectivly informed by use of Twitter data.

In order to use the ```rtweet``` package you must go to Twitter and register an application. That allows you to get an API key which you need in authenticate your session.

```{r}
## load rtweet
library(rtweet)

#============================================================================
# Authentication via Web Browser, tutorial here
# https://rtweet.info/articles/auth.html

## store api keys (these are fake example values; replace with your own keys)
api_key <- "RVNNOQgBNym338HFGBMMlRyyO"
api_secret_key <- "OXkayiLTQRVu8aOvAeZf2bqJs6ErKDPyAeci9mG04OYldwY6Ss"
access_token <- "750554190489985025-MPZa2y4UPmPZ43qqcfnm6PtCP6KwCEA" 
access_secret <- "2NG9SPCQH9W7RsYDIvKUT1O6H810LdRMQ9qsVHMy1kwNk"

## authenticate via web browser

token <- create_token(
  app = "mamultron",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_secret)

#token <- create_token(
#  app = "mamulatron",
#  consumer_key = api_key,
#  consumer_secret = api_secret_key)

## view token (you should see the correct app name)
token
```

```{r}
t <- Sys.time()
## search for some tweets referencing the @NOAA Habitat office.
rt <- search_tweets(
  "@NOAAHabitat",n=10,include_rts = FALSE,retryonratelimit=F)
#=========================================================
Sys.time() -t

head(rt)
```

As you can see, the ```search_tweets()``` methods returns A LOT of information (90 different fields) in the form of a data frame. Something interesting but not terribly complex we can look at to get a feel for what manner of data we have is the screen name and text of each tweet:

```{r}
rt %>% select(screen_name,text)
```

```{r}
# look at the popular hashtags used by people mentioning NOAA Fisheries
noaa.fish <- search_tweets(
  "@NOAAFisheries",n=100,include_rts = FALSE,retryonratelimit=F)
ht <- noaa.fish %>% select(created_at,screen_name,hashtags)

# hashtags are stored in a list tidyr has an answer for that
ht <- unnest(ht,cols=c(hashtags))

# probably too many to visualize so maybe just look at the top 20
top.20 <- ht %>% filter(is.na(hashtags)==F) %>% 
             group_by(hashtags) %>% summarise(count=n()) %>% 
               arrange(-count) %>% filter(row_number() < 21) %>% 
                 arrange(count) %>% mutate(hashtags=factor(hashtags,levels=hashtags))

ggplot(top.20,aes(x=hashtags,y=count)) + geom_bar(stat='identity') + 
  coord_flip() + theme_fivethirtyeight() + 
    ggtitle(label="Popular hashtags in tweets about NOAA Fisheries",
            subtitle="hashtag use last 10 days") +
     theme(plot.title = element_text(size = 14, face = "bold"),
           plot.subtitle = element_text(face = "italic"))
```

## A Text Classification Example {.tabset}

### Intro & Background

In this example I'm going to attempt to illustrate the multinomial Naive Bayes classifier using some text data. The data are tweets sents during the 2016 GOP Primary Debate in Ohio. Each tweet has been classified by hand as "positive", "neutral", or "negative". The goal of this example is to use this supervised data to estimate a predictive model. 

That is, we want to construct a model capable of classifying a tweet as "postive", "negative", or "neutral" based on the content of the tweet.

I can think of a number of reasons why this sort of thing would be cool. One reason I'll propose is the following:

Suppose were're interested in the question, "who won last night's debate?"

One possibility for addressing this question is to gauge who got the most favorable reaction from the twitter-sphere (sure there are lots of problem/caveats with such an approach but conceptually it's decent). And being able to classify a tweet as "positive","negative", or "neutral" would be pretty instrumental to this end. 

The step-by-step for this exercise is something like this:

1. get the pre-classified twitter data
2. do some preliminary cleaning of the text
3. use the ```tm``` package to massage the text data into a nice format
4. split the data into 'training' and 'testing' sets
4. use the 'one hot encoding' approach to define the 'features' of a classification model
5. apply the Naive Bayes algorithm from the ```e1071``` package to the 'training' data
6. use the model to predict sentiment from the 'testing' data

### Set-up and Data

https://uvastatlab.github.io/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/

[Sentiment Analysis Dataset from Kaggle](https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment?select=Sentiment.csv. These are tweets collected during a GOP Debate in Ohio for the 2016 Presidential nomination).

```{r}
tweets <- read.csv('data/Sentiment.csv')

# first I'm going to remove "neutral" tweets to make the example more convincing
#tweets <- tweets %>% filter(sentiment!="Neutral")

# we are going to need the target variable to be categorical
tweets <- tweets %>% mutate(sentiment=factor(sentiment))

head(tweets)

```

### Data Cleaning

This first step is a little hacky. I've experiements with both the ```tidyr``` and ```tm``` packages for working with text mining applications. They both have cool features built-in the clean regular expressions and other complicating things from text data. For some reason, I've had persistent problems with some special characters (like non-ASCII or escape characters). Here I use a simple ```gsub()``` operation to clean a bunch of stuff out of the tweet text.

```{r}
tweets$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "", tweets$text)
```


The "Bag of Words" model will treat each tweet as, well, a big bag of words. Using the ```tm``` package to organize our tweets, one of the first things we do is create a Corpus. A Corpus is a document collection. In this case, each tweet is a document. 

```{r}
corpus <- Corpus(VectorSource(tweets$text))
corpus
```

Now we have a Corpus with 13,871 documents (tweets). 

The ```tm``` package has some built-in methods to remove things like punctuation and stop words. I'll use these to remove some stuff from the Corpus:

```{r}
clean.corpus <- corpus %>% tm_map(tolower) %>%
                   tm_map(removeNumbers) %>%
                       tm_map(removeWords, stopwords()) %>%
                          tm_map(removePunctuation) %>%
                           tm_map(stripWhitespace)

inspect(clean.corpus[1:5])
```

### Document Matricies

In this next part there are a couple things to keep straight:

1. There is a TermDocumentMatrix which orients the terms in the corpus along the rows with documents in the corpus along the columns.

2. Then there is a DocumentTermMatrix which orients the documents along the rows and terms along the columns.

This application is primarily concerned with the DocumentTermMatrix. This is because when applying the Naive Bayes algorithm, it's convenient to have observations (tweets) in the rows and features (words) across the columns.

```{r}
# list of text preparation steps to be applied
# note that we can used individual tm_map() functions above to remove punctuation and numbers...but we can
#    also use this "control" option inside of the term-document-matrix call to remove stopwords, 
#     punctuation, and numbers. My experience was that the "control" option did not work as I had anticipated...
#       specifically, it did not remove some special characters and other stuff that looked weird to me.

# control <- list(stopwords = TRUE, removePunctuation = TRUE, removeNumbers = TRUE) 
#  tdm <- TermDocumentMatrix(clean.corpus, control) 
#  dtm <- DocumentTermMatrix(clean.corpus, control)
```

```{r}
# Create the Document Term Matrix 
dtm <- DocumentTermMatrix(clean.corpus)
inspect(dtm)
```

The regular ```inspect()``` method is not super-informative. But if we are curious about how often some words or phrases occur in our corpus, one thing we can do is pass those terms as a dictionary to the ```DocumentTermMatrix()``` method.

```{r}

inspect(DocumentTermMatrix(clean.corpus,list(dictionary = c("gop", "trump", "cruz"))))
```

Just to follow-up on that. In the output above we see that document 2013 has 1 occurance of the phrase "cruz" and 2 occurances of the phrase "trump". If we look at the orginial tweet text data frame we can see that:

```{r}
tweets$text[1370]
```

### Model Formation

```{r}
# Set up training and testing data using a standard 80/20 split of trainign and testing data

raw.text.train <- tweets[1:round(.8 * nrow(tweets)),]
raw.text.test  <- tweets[(round(.8 * nrow(tweets))+1):nrow(tweets),]

clean.corpus.train <- clean.corpus[1:round(.8 * length(clean.corpus))]
clean.corpus.test  <- clean.corpus[(round(.8 * length(clean.corpus))+1):length(clean.corpus)]

clean.corpus.dtm.train <- dtm[1:round(.8 * nrow(dtm)),]
clean.corpus.dtm.test  <- dtm[(round(.8 * nrow(dtm))+1):nrow(dtm),]
```

Here is an ad-hoc step but one that reduces the impact of infrequently used words. We limit the "bag of words" to words appearing in at least 3 of our documents. 

```{r}
# findFreqTerms finds all terms that are in 3 or more documents in our corpus.
freq.terms <- findFreqTerms(clean.corpus.dtm.train, 3)
clean.corpus.dtm.freq.train <- DocumentTermMatrix(clean.corpus.train, list(dictionary = freq.terms))
clean.corpus.dtm.freq.test  <- DocumentTermMatrix(clean.corpus.test, list(dictionary = freq.terms))

```

This routine changes the elements of the DocumentTermMatrix from word counts to presence/absence. [Here are some good open source notes on NLP](http://spark-public.s3.amazonaws.com/nlp/slides/naivebayes.pdf) from a Stanford course. They include some discussion of the binarized (boolean feature) Naive Bayes algorithm. 

```{r}
convert_counts <- function(x) {
    x <- ifelse(x > 0, 1, 0)
    x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
    return(x)
}

clean.corpus.dtm.freq.train <- apply(clean.corpus.dtm.freq.train, MARGIN = 2, convert_counts)
clean.corpus.dtm.freq.test  <- apply(clean.corpus.dtm.freq.test, MARGIN = 2, convert_counts)
```

Since Naive Bayes evaluates products of probabilities, we need some way of assigning non-zero probabilities to words which do not occur in the sample. We use Laplace 1 smoothing to this end.

```{r}
# Train the classifier
text.classifier <- naiveBayes(clean.corpus.dtm.freq.train, raw.text.train$sentiment,laplace=1)
text.pred <- predict(text.classifier, clean.corpus.dtm.freq.test, type=c("class"))

confusionMatrix(text.pred,raw.text.test$sentiment)

```

## Some Math


## Resources

[A very nice Python-based tutorial on sentiment analysis](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/). 

[Bradley Boehmke](http://uc-r.github.io/creating-text-features) has some cool R tutorials on text mining operations.

Another [Bradley Boehmke](https://uc-r.github.io/tidy_text) tutorial on ```tidy text```.


https://www.kaggle.com/marklvl/sentiment-classification-using-naive-bayes

https://rpubs.com/riazakhan94/naive_bayes_classifier_e1071

https://rpubs.com/cen0te/naivebayes-sentimentpolarity