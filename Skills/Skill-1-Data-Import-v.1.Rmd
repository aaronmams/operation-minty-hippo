---
title: 'Module 4: Data Import v.1'
output:
  html_document:
    df_print: paged
---

```{r include=FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(knitr)
library(kableExtra)
library(data.table)
```
The objective of this module is to get everyone comfortable with importing spreadsheet type data from local sources. My perception (which may or may not be true) of Social Scientists research within NMFS is that it leverages a pretty wide range of data sources. Although I see us moving more in the direction of setting up and utilizing programmatic data streams, many of us still rely (to varying degrees) on data collected under project specific circumstances.

These data are often stored in spreadsheet type formats like .csv, .txt, or .xls. Here we'll walk through some simple functions to get such data into our R enviornments. 

-- read.csv() :: base R
-- fread() :: data.table
-- read_csv() :: tidyverse
-- readlines() :: base R


# Conventionally formatted data

Here we are going to cover 3 methods for importing data from a spreadsheet-like data file that I have saved locally as a .csv. The three methods I'm going to cover are:

* read.csv()
* readr::read_csv()
* data.table::fread()

Here, I'm going to use a fun dataset from Kaggle on the [top 50 most downloaded songs on Spotify from 2019](https://www.kaggle.com/leonardopena/top50spotify2019/version/1#). You can download this .csv from Kaggle or you can go into the [GitHub repository for this project](https://github.com/aaronmams/operation-minty-hippo/blob/master/data/spotify-top50.csv) and download it. Note: downloading a single .csv from a GitHub repository is not super intuitive but if you follow the link and right click on the *Raw* button you will see an option to save the file.

## Conventionally formatted data with read.csv()

The read.csv() method is a base R functionality for reading comma separated value data file formats.

```{r}
songs <- read.csv(file='data/spotify-top50.csv')
head(songs)
```

## Conventionally formatted data with read_csv()

The read_csv() method is part of the tidyr ecosystem and comes from the readr package. According to documentation it's supposed to be a lot faster than read.csv(). [Here is a pretty thorough](https://csgillespie.github.io/efficientR/5-3-importing-data.html) discussion of why/how read_csv() and fread() load large data sets considerably faster than read.csv().

Practically speaking, it works just like the read.csv() method.

```{r}
songs.dplyr <- read_csv(file='data/spotify-top50.csv')
head(songs)
```

## Conventionally formatted data with fread()

The fread() method comes from the data.table package which is a library of functions that work really fast on large data sets.

```{r}
songs.dt <- fread('data/spotify-top50.csv')
head(songs.dt)
```

## Benchmarking Conventionally Formatted Data

A lot of discussion I've seen in R Users Groups centers around the speed/efficiency of the various data import methods for .csv-type data. Here is a visual reproduced from [https://csgillespie.github.io/efficientR/5-3-importing-data.html](https://csgillespie.github.io/efficientR/5-3-importing-data.html)

![](figures/fred_vs_readcsv.png)

Here is a benchmarking example. For benchmarking we'll use a rather large data set from Kaggle. This one is available as part of the [2015 Flight Cancellation Data Mart](https://www.kaggle.com/usdot/flight-delays). The [flight.csv](https://www.kaggle.com/usdot/flight-delays#flights.csv) data file has over 5 million records and 31 fields (columns)

```{r}
#NOTE: THIS WILL TAKE ABOUT 3 MINUTES TO RUN

t <- Sys.time()
flights <- read.csv(file='data/flights.csv')
t.base <- Sys.time() - t


t <- Sys.time()
flights <- read_csv(file='data/flights.csv')
t.dplyr <- Sys.time() - t


t <- Sys.time()
flights <- fread('data/flights.csv')
t.dt <- Sys.time() - t


c(t.base,t.dplyr,t.dt)
```

If you're really into speed and you absolutely must have all 5.8 million records and 31 fields of the data, the fread() method is more than 10X faster than the base read.csv().

## Oddly formatted/poorly formatted 

* readLines()

readlines can be helpful with unstructured or weird data like:

```{r echo=F}
x <- kable(c("John, Smith, Nebraska, 1970-01-05",
        "Jane,Doe,Kansas","Linda,Black,Wisconsin,1974-02-16",
        "Randall,White,1968-11-20"),format='html') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 
gsub("<thead>.*</thead>", "", x)
```

because the 2nd line doesn't have a date and the 4th line doesn't have a state, attempts to use stuff like read.csv() will be problematic because columns don't line up. Here readlines() will read each line separately.

```{r}
dat <- readLines(con='data/readlines-example.txt',warn=F)
str(dat)
```

## Other data read functions of interest

* [read.dta()](https://www.rdocumentation.org/packages/foreign/versions/0.8-75/topics/read.dta): reads STATA .dta datasets
* [readxl](https://readxl.tidyverse.org/): reads excel-type .xls files

It's also worth pointing out that many of the data read functions work with web-hosted data. For example, the NY Times posted a .csv file of county-level COVID-19 cases to a GitHub repository. The url for these data are:

https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv

We can use read.csv with this url:

```{r}
covid.by.county <- read.csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")
```

